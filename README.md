# RAX-nanoGPT: Type-Safe GPT Training with Compile-Time Validation

A JAX/RAX implementation of nanoGPT that transforms "hope it works" into "know it works" through compile-time shape validation, memory safety checks, and explicit type documentation.

## Why RAX-nanoGPT?

Traditional deep learning development is error-prone - shape mismatches, OOM crashes, and unclear tensor dimensions. RAX-nanoGPT solves this with:

### **Compile-Time Safety**
```python
# ❌ Without RAX: Discover shape errors after hours of training
def train_step(params, x, y):  # What shapes? Who knows!
    loss, grads = compute_loss_and_grads(params, x, y)  # RuntimeError after 2 hours!
    return updated_params

# ✅ With RAX: Errors caught in seconds during development  
@rax.validate_function
def train_step(
    params: GPTParams,
    x: Int[Array, "batch seq_len"],     # Crystal clear shapes
    y: Int[Array, "batch seq_len"],     # Self-documenting code
    key: PRNGKeyArray
) -> Tuple[GPTParams, Float[Array, ""], Dict[str, Float[Array, ""]]]:
    # RAX validates shapes at compile time - no surprises!
```

### **Memory Safety**
- **OOM Prevention**: RAX analyzes memory usage before training starts
- **Early Warnings**: Get memory estimates and optimization suggestions  
- **No Wasted Compute**: Bad configurations caught in seconds, not hours

### **Self-Documenting Code**
- **Shape Annotations**: Every tensor parameter documents its expected dimensions
- **IDE Support**: Autocomplete knows exact tensor shapes  
- **Team Onboarding**: New developers understand code structure instantly

### **Zero Performance Overhead**
- **Same Speed**: RAX validation happens at compile time, not runtime
- **JIT Optimized**: Automatic static argument detection for optimal performance
- **Production Ready**: All safety checks with no runtime cost

## Quick Start

### Installation

```bash
# Create virtual environment
uv venv --python 3.13
source .venv/bin/activate

# Install dependencies  
uv add "rax @ git+https://github.com/viraatdas/rax.git" jax jaxlib jaxtyping numpy tiktoken requests tqdm matplotlib
```

### Train Shakespeare Model

```bash
# Prepare data
cd data/shakespeare && python prepare.py && cd ../..

# Train with RAX safety validation
rax run train.py config/train_shakespeare.py
```

**RAX Output:**
```
[RAX] JIT-compiling 'train_step' with static_argnums=[5, 6]
iter 0: loss 10.8160, grad_norm 2.0590, lr 0.00e+00, time 1155.0ms
iter 10: loss 10.5924, grad_norm 2.0571, lr 6.00e-05, time 104.3ms
...
iter 100: loss 6.6116, grad_norm 1.3977, lr 6.00e-04, time 15.3ms
step 100: train loss 6.5919, val loss 6.6791
```

### Generate Text

```bash
rax run sample.py --out_dir=out-shakespeare --start="ROMEO:" --max_new_tokens=200
```

## RAX Safety Features in Action

### 1. Shape Validation
```python
# RAX catches this immediately:
def attention(
    q: Float32[Array, "batch seq d_k"],
    k: Float32[Array, "batch seq d_k"], 
    v: Float32[Array, "batch seq d_v"]  # ← Clear dimension documentation
) -> Float32[Array, "batch seq d_v"]:
    scores = q @ k.T  # RAX validates matrix multiplication compatibility
    return jax.nn.softmax(scores) @ v
```

### 2. Memory Analysis
```bash
# RAX prevents OOM before training starts
[RAX] Analyzing memory usage for 'train_step'...
[RAX] Estimated memory: 8.2 GB (within 16 GB limit) ✅
[RAX] JIT-compiling with optimized static arguments...
```

### 3. Clear Error Messages
```python
# Instead of cryptic PyTorch errors:
# RuntimeError: Expected tensor of size [64, 256] but got [64, 512]

# RAX provides helpful context:
# RAXValidationError: Shape mismatch in 'train_step' parameter 'x'
# Expected: Int[Array, "batch seq_len"] with seq_len=256  
# Got: (64, 512)
# Suggestion: Check your DataLoader block_size configuration
```

## Development Workflow

### With RAX (Recommended)
```bash
# All safety checks enabled
rax run train.py config/my_config.py
```

### Without RAX (For comparison)
```bash  
# Traditional development - shape errors caught at runtime
python train.py config/my_config.py
```

## Performance & Results
TBD

### Benchmark Results
TBD


## Resources

- **Original nanoGPT**: [Andrej Karpathy's implementation](https://github.com/karpathy/nanoGPT)
- **Rax**: [Source code](https://github.com/viraatdas/rax)